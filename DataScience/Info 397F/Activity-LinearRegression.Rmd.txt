---
title: "Activity- Linear Regression"
author: "Gordon Anderson"
output: html_document
---

### Week 3: Algorithms- Linear regression

Linear regression is a technique for modeling a relationship between a set of variables called independent variables, or covariates, or explanatory variables, and a dependent variable, or response, or outcome (a real-valued number). 

It summarizes how the average values of an outcome vary over the subpopulations defined by linear funtions of the covariates. 

### Simple linear regression (one covariate).

The symbolic representation for simple (one covariate) is:
Y = B0 + B1 * X, 
where Y is the respose, X is the covariate, B0 the intercept, B1 the coefficient.

Y is a real-valued number, X can be real-valued or "categorized".
B0 and B1 are real-valued parameters estimated by the way the model is fitted to the data. The usual way this is done is OLS- ordinary least squares- a technique to minimize the errors of all data points relative to a line.

Terms:
residuals- difference between estimated model and data (observed).
errors- difference between true model and data (unobserved) 

B0 is the predicted value when all covariates are zero. The coefficient B1 is the difference predicted when the observed value X changes by one unit.

The model will predict an outcome given a set of explanatory variables. It will also describe the quality of the "linearity" of the data, and give information about the contribution of the covariates to the model, and hence the process we are observing.

Major Assumptions:
- The relationship between the covariates and response is linear.
- All covariates have the same variance.
- The covariates do not interact.

Are these assumptions violated- probably! Do we still use linear regression- oh yeah.

The simplest for of linear regression is with a single independent variable. 

#### Let's look at the "faithful" data set (in base R). 

```{r}
head(faithful)
dim(faithful)
```

The columns:
eruptions Eruption time (in minutes)
waiting   Waiting time to next eruption (in minutes)
Display a scatter plot of the data.

```{r}
plot(eruptions ~ waiting, data=faithful)
```

Looks like two clusters- we'll have to check this out when we do clustering. For now, let's use a linear model. The lm function fits a linear regression model to the data using least squares.
```{r}
fit <- lm(eruptions ~ waiting, data=faithful) 
summary(fit)
```

The summary shows the model intercept and coefficient as well as error bands, significance of the covariate(s) and information about model fit. The coefficient shows the slope of the regression line. For every unit of change in X, the coeffficient show how much the respnse, Y, will change.

Make a plot of the regression line.

```{r}
plot(eruptions ~ waiting, data=faithful)
abline(coef(fit), col = "red", lty=2)
```
Now use a waiting time of 80 and predict the eruption duration. One way to do this is to calculate the result when a specific waiting time is input to the model. The model is: Y = B0 + B1X, where B0 is the intercept and B1 is the coefficient parameter. 

```{r}
attributes(fit)
intercept <- fit$coefficients[[1]]
param <- fit$coefficients[[2]]
```

Now let X be a waiting time of 80 minutes. Calculate the predicted duration.
```{r}
waiting = 80 
# Y = Bo + B1X
duration = intercept + (param * waiting)
duration 
```

Another way to do prediction:

```{r}
newdata = data.frame(waiting=80) 
predict(fit, newdata, interval="predict")
```

Note that the prediction interval is wider than the "confidence" interval. The prediction interval
is used when predicting a random variable, not a model parameter. A random variable is more variable
so the range will be wider.

Topics to explore next:

How do we judge how good the model fits the data?

Look at the output of the fitted model:
```{r}
summary(fit)
```
The fit of this model can be summarized by the model error sd and R-squared.

Another term for errors is "residuals". Residuals are the error between the dependent variable observations, y, and the fitted values B1X:
r = Y - B1X
The residuals are calculated during the model fitting process (usually with OLS- ordinary least squares).
In the output, the Residual standard error is 0.4965.
This is the sd of the errors, or residuals.
```{r}
sd(fit$residuals)
```
In other words, this is the average distance each observation falls from the predicted value. The model can predict the eruption duration to this accuracy. The smaller the better!

R-squared is the fraction of the variance "explained" by the model. The variance "unexplained" by the model is the error sd squared. The ratio of this unexplained variance over the variance of the data subtracted from 1 is R-squared. In the output, R-squared * 100 gives us a percentage of how much of the variance in the data is explained by the model. Note that this is not an absolute measure of goodness of fit. 
If the same model if fitted to a subset of the original data the variance could increase, decreasing R-squared, but we know the model fits the data overall!

A plot of the residuals versus the covariate data should show even dispersal above and below the line.
```{r}
plot(faithful$waiting, fit$residuals, pch=19)
abline(h = 0)
```
Are low R-squared values bad?
Not necessarily.
In some fields, it is entirely expected that your R-squared values will be low. For example, any field that attempts to 
predict human behavior, such as psychology, typically has R-squared values lower than 50%. Humans are simply harder to 
predict than, say, physical processes.

Furthermore, if your R-squared value is low but you have statistically significant predictors, you can still draw important 
conclusions about how changes in the predictor values are associated with changes in the response value. Regardless of the R-squared, 
the significant coefficients still represent the mean change in the response for one unit of change in the predictor while holding 
other predictors in the model constant. Obviously, this type of information can be extremely valuable.

The df, degrees of freedom are calculated:
df = N - c, where N is the data size and c is the number of estimated coefficients, or model parameters.
Note that if c=N the model predicts the data with zero error, no bias, but is not summarizing or generalizing and useless for prediction or inference.

The coefficient(s) estimate includes a standard error(the sd of the error). The ratio of the estimate/sd is the t statistic. The p value of this statistic is how likely it is that the estimate is close to the true value. Note that the significance of the estimate is not the effect of the coefficient on the outcome. 

The p value of the regression model is also important. The overall significance is involved in a hypothesis test with a null hypothesis that the coefficient(s) = 0. The F-statistic is F = explained variance/unexplained variance. The coefficients have a non-zero value if the p value is significant and the null hypothesis is rejected.


### Multiple linear regression

An example with multiple covariates:
outcome: child test scores
covariates: mother's IQ, mother completed HS education

The formula:
kid_score = mom_hs + mom_iq

```{r}
df.child <- read.csv("kidiq_data.csv")
fit.1 <- lm (kid_score ~ mom_hs + mom_iq, data=df.child)
summary(fit.1)
```

The model now is more compilcated. If we hold IQ constant, the model tells us that there is a 6 point difference in outcomes. Alternatively, for the same HS level, changing 1 point in IQ results in a .56 change in outcome score.

What about this fit? A plot of the data using the model with only mothers with HS in the grey points and for mothers who did not complete HS in the black.
The large difference suggests an interaction between the covariates. We are forcing both subpopulations to have the same slope.

```{r}
plot(df.child$mom_iq, df.child$kid_score, xlab="Mother IQ score", 
  ylab="Child test score",pch=20, xaxt="n", yaxt="n", type="n")
curve (coef(fit.1)[1] + coef(fit.1)[2] + coef(fit.1)[3]*x, add=TRUE, col="gray")
curve (coef(fit.1)[1] + coef(fit.1)[3]*x, add=TRUE)
points (df.child$mom_iq[df.child$mom_hs==0], df.child$kid_score[df.child$mom_hs==0], pch=19)
points (df.child$mom_iq[df.child$mom_hs==1], df.child$kid_score[df.child$mom_hs==1], col="gray", pch=19)
axis (1, c(80,100,120,140))
axis (2, c(20,60,100,140))
```

Add an interaction term that allows the model more flexibility to accomodate the interaction.

```{r}
fit.2 <- lm (kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq, data=df.child)
summary(fit.2)
```

Now plot as above but notice the slopes are allowed to vary.

```{r}
plot(df.child$mom_iq, df.child$kid_score, xlab="Mother IQ score",
  ylab="Child test score",pch=20, xaxt="n", yaxt="n", type="n")
curve (coef(fit.2)[1] + coef(fit.2)[2] + (coef(fit.2)[3] + coef(fit.2)[4])*x, add=TRUE, col="gray")
curve (coef(fit.2)[1] + coef(fit.2)[3]*x, add=TRUE)
points (df.child$mom_iq[df.child$mom_hs==0], df.child$kid_score[df.child$mom_hs==0], pch=20)
points (df.child$mom_iq[df.child$mom_hs==1], df.child$kid_score[df.child$mom_hs==1], col="gray", pch=20)
axis (1, c(80,100,120,140))
axis (2, c(20,60,100,140))
```
Point predictions:

```{r}
new <- data.frame (mom_hs=1, mom_iq=100)
predict (fit.1, new, interval="prediction", level=0.95)
predict (fit.2, new, interval="prediction", level=0.95)
```
